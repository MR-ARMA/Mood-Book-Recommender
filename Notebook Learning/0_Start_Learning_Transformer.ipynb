{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start to Install requierments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- install requierments libraries\n",
    "- Set Git  & Github\n",
    "- Create a Basic Project Structure\n",
    "- Install & Create conda env to start learning and  testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Understanding Transformers and Self-Attention Mechanisms: A Comprehensive Training Guide**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Introduction to Transformers](#1-introduction-to-transformers)\n",
    "2. [Word Embeddings](#2-word-embeddings)\n",
    "3. [Positional Encoding](#3-positional-encoding)\n",
    "4. [Self-Attention Mechanism](#4-self-attention-mechanism)\n",
    "    - [4.1. Overview](#41-overview)\n",
    "    - [4.2. Query, Key, and Value Matrices](#42-query-key-and-value-matrices)\n",
    "    - [4.3. Calculating Attention Scores](#43-calculating-attention-scores)\n",
    "    - [4.4. Applying Softmax](#44-applying-softmax)\n",
    "    - [4.5. Generating the Output](#45-generating-the-output)\n",
    "5. [Numerical Example of Self-Attention](#5-numerical-example-of-self-attention)\n",
    "    - [5.1. Step 1: Define the Input Sentence and Embeddings](#51-step-1-define-the-input-sentence-and-embeddings)\n",
    "    - [5.2. Step 2: Define Query, Key, and Value Matrices](#52-step-2-define-query-key-and-value-matrices)\n",
    "    - [5.3. Step 3: Calculate Query, Key, and Value Vectors](#53-step-3-calculate-query-key-and-value-vectors)\n",
    "    - [5.4. Step 4: Calculate Attention Scores](#54-step-4-calculate-attention-scores)\n",
    "    - [5.5. Step 5: Compute the Output](#55-step-5-compute-the-output)\n",
    "6. [Detailed Explanation of the Output](#6-detailed-explanation-of-the-output)\n",
    "7. [Final Thoughts](#7-final-thoughts)\n",
    "8. [Glossary](#8-glossary)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction to Transformers**\n",
    "\n",
    "Transformers have revolutionized the field of natural language processing (NLP) by enabling models to handle long-range dependencies in data more effectively than previous architectures like recurrent neural networks (RNNs) or convolutional neural networks (CNNs). Introduced in the paper [\"Attention Is All You Need\"](/Docs/references/NIPS-2017-attention-is-all-you-need-Paper.pdf) by Vaswani et al., Transformers leverage a mechanism known as **self-attention** to process input data in parallel, making them highly efficient and scalable.\n",
    "\n",
    "### **Key Components of Transformers:**\n",
    "\n",
    "- **Encoder-Decoder Structure:** Transformers typically consist of an encoder that processes the input and a decoder that generates the output.\n",
    "- **Self-Attention Mechanism:** Allows the model to weigh the importance of different parts of the input data dynamically.\n",
    "- **Multi-Head Attention:** Enhances the model's ability to focus on different representation subspaces.\n",
    "- **Positional Encoding:** Introduces information about the order of the sequence since Transformers lack inherent sequential processing.\n",
    "\n",
    "This guide focuses on the **self-attention mechanism**, a core component of Transformers, detailing how it operates and providing a numerical example to illustrate its functionality.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Word Embeddings**\n",
    "\n",
    "Before delving into self-attention, it's essential to understand how words are represented numerically within Transformer models.\n",
    "\n",
    "### **What Are Word Embeddings?**\n",
    "\n",
    "Word embeddings are dense vector representations of words in a continuous vector space where semantically similar words are mapped to nearby points. They capture syntactic and semantic information about words, enabling models to understand relationships and contexts.\n",
    "\n",
    "### **Common Methods for Generating Embeddings:**\n",
    "\n",
    "- **Pre-trained Embeddings:** Models like Word2Vec, GloVe, and FastText provide pre-trained embeddings based on large corpora.\n",
    "- **Learned Embeddings:** In Transformer architectures, embeddings are often learned during the training process, allowing them to be fine-tuned for specific tasks.\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "Consider the sentence: **\"I love AI\"**\n",
    "\n",
    "Each word is mapped to a **d-dimensional** vector. For simplicity, we'll use 3-dimensional vectors in this example:\n",
    "\n",
    "- **I** → [1, 0, 1]\n",
    "- **love** → [0, 1, 0]\n",
    "- **AI** → [1, 1, 0]\n",
    "\n",
    "Thus, the input embedding matrix $ X $ (3 words × 3 dimensions) is:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Positional Encoding**\n",
    "\n",
    "Transformers process input data in parallel, lacking inherent sequential information. To inject information about the order of words in a sequence, **positional encodings** are added to the word embeddings.\n",
    "\n",
    "### **Why Positional Encoding?**\n",
    "\n",
    "Without positional encoding, the model treats the input as a bag of words, ignoring the order, which is crucial for understanding context and meaning.\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "Positional encodings are vectors added to the word embeddings, encoding the position of each word in the sequence. These encodings can be:\n",
    "\n",
    "- **Fixed:** Using sine and cosine functions of different frequencies.\n",
    "- **Learned:** Parameters learned during training.\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "Continuing with our sentence **\"I love AI\"**, suppose we add simple positional encodings:\n",
    "\n",
    "- **I** (position 1) → [1, 0, 1] + [0.1, 0.2, 0.3] = [1.1, 0.2, 1.3]\n",
    "- **love** (position 2) → [0, 1, 0] + [0.2, 0.3, 0.4] = [0.2, 1.3, 0.4]\n",
    "- **AI** (position 3) → [1, 1, 0] + [0.3, 0.4, 0.5] = [1.3, 1.4, 0.5]\n",
    "\n",
    "The updated embedding matrix with positional encodings becomes:\n",
    "\n",
    "$$\n",
    "\\text{Embedding matrix} =\n",
    "\\begin{bmatrix}\n",
    "1.1 & 0.2 & 1.3 \\\\\n",
    "0.2 & 1.3 & 0.4 \\\\\n",
    "1.3 & 1.4 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Self-Attention Mechanism**\n",
    "\n",
    "Self-attention allows the model to weigh the relevance of different words in a sequence relative to each other, enabling the model to capture dependencies irrespective of their distance in the sequence.\n",
    "\n",
    "### **4.1. Overview**\n",
    "\n",
    "In the self-attention mechanism:\n",
    "\n",
    "- **Each word** in the input sequence is transformed into three vectors: **Query (Q)**, **Key (K)**, and **Value (V)**.\n",
    "- **Attention scores** are calculated using the dot product of Query and Key vectors.\n",
    "- These scores determine how much attention each word should pay to others.\n",
    "- The final output is a weighted sum of the Value vectors based on these attention scores.\n",
    "\n",
    "### **4.2. Query, Key, and Value Matrices**\n",
    "\n",
    "**Query (Q):** Represents the word seeking information.\n",
    "\n",
    "**Key (K):** Represents the word providing information.\n",
    "\n",
    "**Value (V):** Contains the actual information to be aggregated.\n",
    "\n",
    "These matrices are obtained by applying learned linear transformations to the input embeddings.\n",
    "\n",
    "### **4.3. Calculating Attention Scores**\n",
    "\n",
    "Attention scores are computed by taking the dot product of the Query vector of one word with the Key vectors of all words, including itself. These scores indicate the relevance of each word to the Query word.\n",
    "\n",
    "### **4.4. Applying Softmax**\n",
    "\n",
    "To convert raw attention scores into probabilities, the **softmax** function is applied. This ensures that the attention weights sum to 1, allowing them to be interpreted as probabilities.\n",
    "\n",
    "### **4.5. Generating the Output**\n",
    "\n",
    "The final output for each word is a weighted sum of the Value vectors, where the weights are the attention probabilities. This output incorporates contextual information from the entire sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Numerical Example of Self-Attention**\n",
    "\n",
    "To elucidate the self-attention mechanism, let's walk through a detailed numerical example using the sentence **\"I love AI\"**.\n",
    "\n",
    "### **5.1. Step 1: Define the Input Sentence and Embeddings**\n",
    "\n",
    "Assume the sentence: **\"I love AI\"**.\n",
    "\n",
    "Each word is assigned a 3-dimensional embedding vector:\n",
    "\n",
    "- **I** → [1, 0, 1]\n",
    "- **love** → [0, 1, 0]\n",
    "- **AI** → [1, 1, 0]\n",
    "\n",
    "The input embedding matrix $ X $ is:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **5.2. Step 2: Define Query, Key, and Value Matrices**\n",
    "\n",
    "Define the weight matrices $ W_Q $, $ W_K $, and $ W_V $ (all 3×3 for simplicity):\n",
    "\n",
    "$$\n",
    "W_Q = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    ",\\quad\n",
    "W_K = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    ",\\quad\n",
    "W_V = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These matrices are **learned** during the training process and are used to project the input embeddings into the Query, Key, and Value spaces.\n",
    "\n",
    "### **5.3. Step 3: Calculate Query, Key, and Value Vectors**\n",
    "\n",
    "Compute $ Q $, $ K $, and $ V $ by multiplying the input embeddings $ X $ with the respective weight matrices.\n",
    "\n",
    "#### **Query Matrix (Q):**\n",
    "\n",
    "$$\n",
    "Q = X \\times W_Q = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "(1\\cdot1 + 0\\cdot0 + 1\\cdot1) & (1\\cdot0 + 0\\cdot1 + 1\\cdot0) & (1\\cdot1 + 0\\cdot0 + 1\\cdot1) \\\\\n",
    "(0\\cdot1 + 1\\cdot0 + 0\\cdot1) & (0\\cdot0 + 1\\cdot1 + 0\\cdot0) & (0\\cdot1 + 1\\cdot0 + 0\\cdot1) \\\\\n",
    "(1\\cdot1 + 1\\cdot0 + 0\\cdot1) & (1\\cdot0 + 1\\cdot1 + 0\\cdot0) & (1\\cdot1 + 1\\cdot0 + 0\\cdot1)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 2 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Key Matrix (K):**\n",
    "\n",
    "$$\n",
    "K = X \\times W_K = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "(1\\cdot1 + 0\\cdot0 + 1\\cdot1) & (1\\cdot1 + 0\\cdot1 + 1\\cdot0) & (1\\cdot0 + 0\\cdot1 + 1\\cdot1) \\\\\n",
    "(0\\cdot1 + 1\\cdot0 + 0\\cdot1) & (0\\cdot1 + 1\\cdot1 + 0\\cdot0) & (0\\cdot0 + 1\\cdot1 + 0\\cdot1) \\\\\n",
    "(1\\cdot1 + 1\\cdot0 + 0\\cdot1) & (1\\cdot1 + 1\\cdot1 + 0\\cdot0) & (1\\cdot0 + 1\\cdot1 + 0\\cdot1)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 & 1 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Value Matrix (V):**\n",
    "\n",
    "$$\n",
    "V = X \\times W_V = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "(1\\cdot1 + 0\\cdot0 + 1\\cdot0) & (1\\cdot0 + 0\\cdot1 + 1\\cdot0) & (1\\cdot1 + 0\\cdot0 + 1\\cdot1) \\\\\n",
    "(0\\cdot1 + 1\\cdot0 + 0\\cdot0) & (0\\cdot0 + 1\\cdot1 + 0\\cdot0) & (0\\cdot1 + 1\\cdot0 + 0\\cdot1) \\\\\n",
    "(1\\cdot1 + 1\\cdot0 + 0\\cdot0) & (1\\cdot0 + 1\\cdot1 + 0\\cdot0) & (1\\cdot1 + 1\\cdot0 + 0\\cdot1)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **5.4. Step 4: Calculate Attention Scores**\n",
    "\n",
    "Attention scores determine how much focus each word should have on others. They are computed using the dot product of the Query matrix $ Q $ with the transpose of the Key matrix $ K^T $, scaled by the square root of the key dimension $ d_k $.\n",
    "\n",
    "#### **Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Attention scores} = \\frac{Q \\times K^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Given $ d_k = 3 $, $ \\sqrt{d_k} \\approx 1.732 $.\n",
    "\n",
    "#### **Calculating** $ Q \\times K^T $:\n",
    "\n",
    "$$\n",
    "K^T = \n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 1 \\\\\n",
    "1 & 1 & 2 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    ",\\quad\n",
    "Q \\times K^T =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 2 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 1 \\\\\n",
    "1 & 1 & 2 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "(2\\cdot2 + 0\\cdot1 + 2\\cdot1) & (2\\cdot0 + 0\\cdot1 + 2\\cdot1) & (2\\cdot1 + 0\\cdot2 + 2\\cdot1) \\\\\n",
    "(0\\cdot2 + 1\\cdot1 + 0\\cdot1) & (0\\cdot0 + 1\\cdot1 + 0\\cdot1) & (0\\cdot1 + 1\\cdot2 + 0\\cdot1) \\\\\n",
    "(1\\cdot2 + 1\\cdot1 + 1\\cdot1) & (1\\cdot0 + 1\\cdot1 + 1\\cdot1) & (1\\cdot1 + 1\\cdot2 + 1\\cdot1)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6 & 2 & 4 \\\\\n",
    "1 & 1 & 2 \\\\\n",
    "4 & 2 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Scaling by  \\sqrt{d_k} :**\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{3}} \\times\n",
    "\\begin{bmatrix}\n",
    "6 & 2 & 4 \\\\\n",
    "1 & 1 & 2 \\\\\n",
    "4 & 2 & 3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3.46 & 1.15 & 2.31 \\\\\n",
    "0.58 & 0.58 & 1.15 \\\\\n",
    "2.31 & 1.15 & 1.73\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **5.5. Step 5: Compute the Output**\n",
    "\n",
    "The final output is obtained by multiplying the attention scores (after applying softmax) with the Value matrix $ V $.\n",
    "\n",
    "#### **Applying Softmax:**\n",
    "\n",
    "Softmax is applied row-wise to normalize the attention scores into probabilities.\n",
    "\n",
    "**Example: Softmax for the First Row:**\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(3.46, 1.15, 2.31) = \\left( \\frac{e^{3.46}}{e^{3.46} + e^{1.15} + e^{2.31}}, \\frac{e^{1.15}}{e^{3.46} + e^{1.15} + e^{2.31}}, \\frac{e^{2.31}}{e^{3.46} + e^{1.15} + e^{2.31}} \\right)\n",
    "$$\n",
    "\n",
    "Assuming the softmax results for simplicity:\n",
    "\n",
    "$$\n",
    "\\text{Attention scores} =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.1 & 0.3 \\\\\n",
    "0.2 & 0.5 & 0.3 \\\\\n",
    "0.3 & 0.2 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Multiplying by Value Matrix V :**\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Attention scores} \\times V =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.1 & 0.3 \\\\\n",
    "0.2 & 0.5 & 0.3 \\\\\n",
    "0.3 & 0.2 & 0.5\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "(0.6\\cdot1 + 0.1\\cdot0 + 0.3\\cdot1) & (0.6\\cdot0 + 0.1\\cdot1 + 0.3\\cdot1) & (0.6\\cdot2 + 0.1\\cdot0 + 0.3\\cdot1) \\\\\n",
    "(0.2\\cdot1 + 0.5\\cdot0 + 0.3\\cdot1) & (0.2\\cdot0 + 0.5\\cdot1 + 0.3\\cdot1) & (0.2\\cdot2 + 0.5\\cdot0 + 0.3\\cdot1) \\\\\n",
    "(0.3\\cdot1 + 0.2\\cdot0 + 0.5\\cdot1) & (0.3\\cdot0 + 0.2\\cdot1 + 0.5\\cdot1) & (0.3\\cdot2 + 0.2\\cdot0 + 0.5\\cdot1)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1.3 & 0.4 & 1.5 \\\\\n",
    "0.5 & 0.8 & 1.2 \\\\\n",
    "0.8 & 0.7 & 1.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The **output matrix** represents the **self-attended embeddings** for each word in the sentence, incorporating contextual information from the entire sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Detailed Explanation of the Output**\n",
    "\n",
    "The output of the self-attention mechanism transforms each word's embedding by incorporating information from other words in the sequence, weighted by the attention scores. This process enables the model to generate **contextualized representations** of each word, which are essential for understanding nuances in language.\n",
    "\n",
    "### **Understanding the Output Matrix:**\n",
    "\n",
    "$$\n",
    "\\text{Output} = \n",
    "\\begin{bmatrix}\n",
    "1.3 & 0.4 & 1.5 \\quad (\\text{for \"I\"}) \\\\\n",
    "0.5 & 0.8 & 1.2 \\quad (\\text{for \"love\"}) \\\\\n",
    "0.8 & 0.7 & 1.5 \\quad (\\text{for \"AI\"})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Interpretation:**\n",
    "\n",
    "- **Output for \"I\":** [1.3, 0.4, 1.5]\n",
    "    - **0.6** of \"I\"'s representation comes from itself.\n",
    "    - **0.1** comes from \"love.\"\n",
    "    - **0.3** comes from \"AI.\"\n",
    "\n",
    "- **Output for \"love\":** [0.5, 0.8, 1.2]\n",
    "    - **0.2** from \"I.\"\n",
    "    - **0.5** from itself.\n",
    "    - **0.3** from \"AI.\"\n",
    "\n",
    "- **Output for \"AI\":** [0.8, 0.7, 1.5]\n",
    "    - **0.3** from \"I.\"\n",
    "    - **0.2** from \"love.\"\n",
    "    - **0.5** from itself.\n",
    "\n",
    "### **Significance of the Output:**\n",
    "\n",
    "- **Contextual Awareness:** Each word's embedding now contains information about other words in the sentence, enabling the model to grasp context more effectively.\n",
    "- **Dynamic Weighting:** The attention mechanism dynamically adjusts the influence of each word based on relevance, allowing the model to focus on pertinent information.\n",
    "- **Enhanced Representations:** These enriched embeddings serve as inputs to subsequent layers in the Transformer, facilitating deeper understanding and more accurate predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Final Thoughts**\n",
    "\n",
    "The self-attention mechanism is a cornerstone of Transformer architectures, empowering models to process and understand language with unprecedented efficiency and accuracy. By dynamically weighing the importance of each word in a sequence relative to others, self-attention facilitates the creation of rich, contextualized representations that are vital for a wide array of NLP tasks, including translation, summarization, and question-answering.\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "- **Parallel Processing:** Unlike sequential models, Transformers process all words simultaneously, enhancing computational efficiency.\n",
    "- **Scalability:** The architecture scales well with large datasets and complex tasks.\n",
    "- **Versatility:** Self-attention mechanisms are not limited to NLP and can be applied to other domains like computer vision.\n",
    "\n",
    "Understanding the intricacies of self-attention and its implementation within Transformers is essential for leveraging these models effectively in various applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Glossary**\n",
    "\n",
    "- **Attention Mechanism:** A technique that allows models to focus on specific parts of the input data when generating each part of the output.\n",
    "- **Embedding Vector:** A numerical representation of a word in a continuous vector space.\n",
    "- **Query (Q):** A vector representing the current word seeking information.\n",
    "- **Key (K):** A vector representing a word that provides information.\n",
    "- **Value (V):** A vector containing the actual information to be aggregated.\n",
    "- **Softmax:** A function that converts a vector of values into probabilities that sum to 1.\n",
    "- **Positional Encoding:** Information added to embeddings to encode the position of words in a sequence.\n",
    "- **Multi-Head Attention:** An extension of self-attention that allows the model to focus on different representation subspaces.\n",
    "- **Transformer:** A neural network architecture that relies entirely on self-attention mechanisms to process input data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 1. Data Preparation\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.labels[idx]\n",
    "\n",
    "# Sample data (in a real project, we use a larger dataset)\n",
    "reviews = [\n",
    "    \"I loved this movie, it was fantastic!\",\n",
    "    \"Terrible film, complete waste of time\",\n",
    "    \"Great acting, but the plot was weak\",\n",
    "    \"Absolutely fantastic, highly recommended\",\n",
    "    \"Boring and predictable, don't bother\",\n",
    "    \"A masterpiece of modern cinema\",\n",
    "    \"The worst movie I've ever seen\",\n",
    "    \"Decent film, worth watching once\",\n",
    "    \"Incredible special effects, mediocre story\",\n",
    "    \"A classic that never gets old\"\n",
    "]\n",
    "labels = [1, 0, 1, 1, 0, 1, 0, 1, 1, 1]  # 1 for positive, 0 for negative\n",
    "\n",
    "print(\"Original Data:\")\n",
    "for review, label in zip(reviews, labels):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {'Positive' if label == 1 else 'Negative'}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create vocabulary\n",
    "vocab = set(\" \".join(reviews).split())\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(\"Sample of vocabulary:\")\n",
    "print(list(word_to_idx.items())[:10])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert reviews to sequences of indices\n",
    "reviews_idx = [[word_to_idx[word] for word in review.split()] for review in reviews]\n",
    "max_length = max(len(review) for review in reviews_idx)\n",
    "\n",
    "# Pad sequences\n",
    "reviews_padded = [review + [vocab_size] * (max_length - len(review)) for review in reviews_idx]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "reviews_tensor = torch.LongTensor(reviews_padded)\n",
    "labels_tensor = torch.FloatTensor(labels)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_tensor, labels_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = SentimentDataset(X_train, y_train)\n",
    "test_dataset = SentimentDataset(X_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Self-Attention Module\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        \n",
    "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisible by heads\"\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        \n",
    "    def forward(self, values, keys, queries):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "        \n",
    "        # Split the embedding into self.heads pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        \n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "        \n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, value, key, query):\n",
    "        attention = self.attention(value, key, query)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. Sentiment Classifier\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length\n",
    "    ):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size + 1, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out)\n",
    "        \n",
    "        out = self.fc_out(out[:, 0, :])\n",
    "        return torch.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Training and Evaluation Functions\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_reviews, batch_labels in dataloader:\n",
    "        batch_reviews, batch_labels = batch_reviews.to(device), batch_labels.to(device)\n",
    "        \n",
    "        outputs = model(batch_reviews).squeeze(1)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_reviews, batch_labels in dataloader:\n",
    "            batch_reviews, batch_labels = batch_reviews.to(device), batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_reviews).squeeze(1)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Model Setup and Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SentimentClassifier(\n",
    "    vocab_size + 1,  # +1 for padding token\n",
    "    embed_size=64,\n",
    "    num_layers=2,\n",
    "    heads=4,\n",
    "    device=device,\n",
    "    forward_expansion=4,\n",
    "    dropout=0.1,\n",
    "    max_length=max_length\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "    test_loss, test_accuracy = evaluate(model, test_dataloader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Test Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_accuracies)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results.png')\n",
    "print(\"Training results plot saved as 'training_results.png'\")\n",
    "\n",
    "# 7. Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_reviews = [\n",
    "        \"This movie was absolutely amazing\",\n",
    "        \"I didn't enjoy this film at all\",\n",
    "        \"The acting was great but the story was confusing\",\n",
    "        \"A must-watch for all cinema lovers\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTesting the model with new reviews:\")\n",
    "    for test_review in test_reviews:\n",
    "        print(f\"\\nInput: {test_review}\")\n",
    "        test_review_idx = [word_to_idx.get(word, vocab_size) for word in test_review.split()]\n",
    "        test_review_padded = test_review_idx + [vocab_size] * (max_length - len(test_review_idx))\n",
    "        test_review_tensor = torch.LongTensor(test_review_padded).unsqueeze(0).to(device)\n",
    "        \n",
    "        output = model(test_review_tensor)\n",
    "        sentiment = 'Positive' if output.item() > 0.5 else 'Negative'\n",
    "        confidence = output.item() if sentiment == 'Positive' else 1 - output.item()\n",
    "        \n",
    "        print(f\"Output: {output.item():.4f}\")\n",
    "        print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence:.2%})\")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'sentiment_classifier.pth')\n",
    "print(\"\\nModel saved as 'sentiment_classifier.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
